{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<staticmethod object at 0x7f65bd0c6558>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "utils.py:7: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 499, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/tornado/ioloop.py\", line 1073, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-a6eb00eb9a79>\", line 3, in <module>\n",
      "    import CalculateCoElutionScores as CS\n",
      "  File \"CalculateCoElutionScores.py\", line 4, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/home/kuan-hao/miniconda3/envs/EPIC/lib/python2.7/site-packages/matplotlib/backends/__init__.py\", line 17, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import CalculateCoElutionScores as CS\n",
    "import GoldStandard as GS\n",
    "import utils as utils\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = '000000001'\n",
    "input_dir = '/home/kuan-hao/EPPC/input/real_data/elution_profiles_test/'\n",
    "taxid = ''\n",
    "ppi = ''\n",
    "cluster = '/home/kuan-hao/EPPC/input/real_data/gold_standard.tsv'\n",
    "output_dir_ = '/home/kuan-hao/EPPC/output/TEST/'\n",
    "output_prefix = 'TEST'\n",
    "classifier = 'CNN'\n",
    "num_cores = 6\n",
    "mode = 'EXP'\n",
    "fun_anno_source = 'STRING'\n",
    "co_elution_cutoff = 0.5\n",
    "classifier_cutoff = 0.5\n",
    "elution_max_count = 1\n",
    "frac_count = 2\n",
    "precalcualted_score_file = \"NONE\"\n",
    "K_D_TRAIN = 'd'\n",
    "FOLD_NUM = 5\n",
    "TRAIN_TEST_RATIO = 0.3\n",
    "POS_NEG_RATIO = 1\n",
    "NUM_EP = 2\n",
    "NUM_FRC = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this_scores: ', [<CalculateCoElutionScores.Raw_eps instance at 0x7f65b8201998>])\n",
      "Raw_eps\n"
     ]
    }
   ],
   "source": [
    "if feature_selection == \"00000000\":\n",
    "    print \"Select at least one feature\"\n",
    "    sys.exit()\n",
    "elif feature_selection == \"000000000\":\n",
    "    this_scores = utils.get_fs_comb(feature_selection)\n",
    "    print \"\\t\".join([fs.name for fs in this_scores])\n",
    "elif feature_selection == \"000000001\":\n",
    "    this_scores = utils.get_fs_comb(feature_selection)\n",
    "    print \"\\t\".join([fs.name for fs in this_scores])\n",
    "else:\n",
    "    this_scores = utils.get_fs_comb(feature_selection)\n",
    "    print \"\\t\".join([fs.name for fs in this_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing /home/kuan-hao/EPPC/input/real_data/elution_profiles_test/intensity_high_500.tsv\n",
      " removed 23 (0.05, total: 499, after filtering: 476) proteins found in less than 2 fraction\n"
     ]
    }
   ],
   "source": [
    "foundprots, elution_datas = utils.load_data(input_dir, this_scores, fc=frac_count, mfc=elution_max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complexes from file\n",
      "('&& args.cluster: ', '/home/kuan-hao/EPPC/input/real_data/gold_standard.tsv')\n",
      "('&& args.mode: ', 'EXP')\n",
      "Average size of predicted complexes is: 4.84695652174\n",
      "[<GoldStandard.FileClusters instance at 0x7f65b8206320>]\n",
      "Total number of complexes 190 in Goldstandard\n",
      "Number of complexes after ortholog mapping 190 complexes in Goldstandard\n",
      "After removing not indetified proteins 190 number of complexes in Goldstandard\n",
      "After size filtering 30 number of complexes in Goldstandard\n",
      "After mergning 28 number of complexes in Goldstandard\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "## \"Generate reference data set\"\n",
    "##########################################\n",
    "gs = \"\"\n",
    "if ((taxid != \"\" and  ppi != \"\") or (cluster != \"\" and  ppi != \"\" )):\n",
    "    print \"Refernce from cluster and PPI are nor compatiple. Please supply ppi or complex reference, not both!\"\n",
    "    sys.exit()\n",
    "\n",
    "if taxid == \"\" and  ppi == \"\" and cluster == \"\":\n",
    "    print \"Please supply a reference by setting taxid, cluster, or ppi tag\"\n",
    "    sys.exit()\n",
    "\n",
    "gs_clusters = []\n",
    "if (taxid != \"\" and cluster == \"\" and ppi == \"\"):\n",
    "    print \"Loading clusters from GO, CORUM, and Intact\"\n",
    "    gs_clusters.extend(utils.get_reference_from_net(taxid))\n",
    "\n",
    "if cluster != \"\":\n",
    "    print \"Loading complexes from file\"\n",
    "    print(\"&& args.cluster: \", cluster)\n",
    "    print(\"&& args.mode: \", mode)\n",
    "    if mode == \"FA\":\n",
    "        gs_clusters.append(GS.FileClusters(cluster, \"all\"))\n",
    "    else:\n",
    "        gs_clusters.append(GS.FileClusters(cluster, foundprots))\n",
    "\n",
    "\n",
    "if ppi != \"\":\n",
    "    print \"Reading PPI file from %s\" % reference\n",
    "    gs = Goldstandard_from_PPI_File(ppi, foundprots)\n",
    "\n",
    "print gs_clusters\n",
    "if \tlen(gs_clusters)>0:\n",
    "    gs = utils.create_goldstandard(gs_clusters, taxid, foundprots, pos_neg_ratio = POS_NEG_RATIO)\n",
    "\n",
    "\n",
    "output_dir = output_dir_ + os.sep + output_prefix\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "refFH = open(output_dir + \".ref_complexes.txt\", \"w\")\n",
    "for comp in gs.complexes.complexes:\n",
    "        print >> refFH, \"%s\\t%s\" % (\",\".join(comp), \",\".join(gs.complexes.complexes[comp]))\n",
    "refFH.close()\n",
    "##########################################\n",
    "## End of \"Generate reference data set\"\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CNN\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "## \"Creating classifier\"\n",
    "##########################################\n",
    "classifier_select = classifier\n",
    "clf = CS.CLF_Wrapper(num_cores, classifier_select, num_ep=NUM_EP, num_frc=NUM_FRC, pos_neg_ratio = POS_NEG_RATIO)\n",
    "##########################################\n",
    "## End of \"Creating classifier\"\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering: intensity_high_500.tsv\n",
      "Before filtering 113050 PPIs\n",
      "filtering\n",
      "After filtering 81609 PPIs\n",
      "Num of PPIs across all data sets after filtering 81609\n",
      "('num_rows: ', 5671)\n",
      "('len(self.features): ', 1)\n",
      "('num_frc: ', 27)\n",
      "done calcualting co-elution scores\n",
      "('** self.scores: ', array([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.66223253, 0.12831917, 0.04597004, ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.02134712, 0.01442621, 0.04481638, ..., 0.00409582,\n",
      "         0.00422245, 0.00711002],\n",
      "        [0.59756116, 0.28238223, 0.12005661, ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.31466232, 0.42925407, 0.18827869, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.01416298, 0.0016486 , 0.00148367, ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.        , 0.        , 0.03770384, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.17714545, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.00931485, 0.00074149, 0.        , ..., 0.00716776,\n",
      "         0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]],\n",
      "\n",
      "       [[0.01822498, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ]]]))\n",
      "('scoreCalc.scores.shape: ', (5012, 2, 27))\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "## Calculating coelutionScores\n",
    "##########################################\n",
    "scoreCalc = CS.CalculateCoElutionScores(this_scores, elution_datas, output_dir + \".scores.txt\", num_cores=num_cores, cutoff= co_elution_cutoff)\n",
    "if precalcualted_score_file == \"NONE\":\n",
    "    scoreCalc.calculate_coelutionDatas(gs)\n",
    "else:\n",
    "    scoreCalc.readTable(precalcualted_score_file, gs)\n",
    "\n",
    "print(\"scoreCalc.scores.shape: \", scoreCalc.scores.shape)\n",
    "\n",
    "## Use autoencoder to extract features\n",
    "\n",
    "##########################################\n",
    "## End of \"Calculating coelutionScores\"\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('len(gs.positive): ', 244)\n",
      "('len(gs.negative): ', 4768)\n",
      "('len(gs.all_positive): ', 244)\n",
      "('len(gs.all_negative): ', 4768)\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "## \"Balancing Positive & Negative\" PPIs\n",
    "##########################################\n",
    "functionalData = \"\"\n",
    "gs.positive = set(gs.positive & set(scoreCalc.ppiToIndex.keys()))\n",
    "gs.negative = set(gs.negative & set(scoreCalc.ppiToIndex.keys()))\n",
    "gs.all_positive = gs.positive\n",
    "gs.all_negative = gs.negative\n",
    "# gs.rebalance()\n",
    "\n",
    "print(\"len(gs.positive): \", len(gs.positive))\n",
    "print(\"len(gs.negative): \", len(gs.negative))\n",
    "\n",
    "print(\"len(gs.all_positive): \", len(gs.all_positive))\n",
    "print(\"len(gs.all_negative): \", len(gs.all_negative))\n",
    "###############################################\n",
    "## End of \"Balancing Positive & Negative\" PPIs\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('&& self.scores.shape[0]: ', 5012)\n",
      "('&& pos_count: ', 244)\n",
      "('&& neg_count: ', 4768)\n",
      "('&& self.scores.shape[0]: ', 5012)\n",
      "('&& pos_count_all: ', 244)\n",
      "('&& neg_count_all: ', 4768)\n",
      "('&& unsure_count_all: ', 0)\n",
      "('data.shape', (5012, 2, 27))\n",
      "('targets.shape', (5012,))\n",
      "('data_all.shape', (5012, 2, 27))\n",
      "('targets_all.shape', (5012,))\n"
     ]
    }
   ],
   "source": [
    "_, data, targets, _all, data_all, targets_all = scoreCalc.toSklearnData(gs)\n",
    "print(\"data.shape\", data.shape)\n",
    "print(\"targets.shape\", targets.shape)\n",
    "# print(\"_all.shape\", _all.shape)\n",
    "print(\"data_all.shape\", data_all.shape)\n",
    "print(\"targets_all.shape\", targets_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data: ', (5012, 2, 27))\n",
      "('data.shape: ', (5012, 54))\n",
      "[0.         0.         0.         0.         0.         0.48754491\n",
      " 0.51245509 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.66223253 0.12831917 0.04597004\n",
      " 0.0683785  0.04115505 0.05033422 0.0036105  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "('targets.shape: ', (5012,))\n",
      "('     x_train: ', (3508, 54))\n",
      "('     x_test: ', (1504, 54))\n",
      "('     y_train: ', (3508,))\n",
      "('     y_test: ', (1504,))\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "num_samples, num_scores, num_frc = data.shape\n",
    "print(\"data: \", data.shape)\n",
    "data = data.reshape((num_samples, num_scores*num_frc))\n",
    "print(\"data.shape: \", data.shape)\n",
    "print(data[0])\n",
    "print(\"targets.shape: \", targets.shape)\n",
    "# print(targets)\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, targets, test_size=0.3, random_state=1)\n",
    "print(\"     x_train: \", x_train.shape)\n",
    "print(\"     x_test: \", x_test.shape)\n",
    "print(\"     y_train: \", y_train.shape)\n",
    "print(\"     y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_img = keras.Input(shape=(54,))\n",
    "# encoded = layers.Dense(27, activation='relu')(input_img)\n",
    "# encoded = layers.Dense(14, activation='relu')(encoded)\n",
    "# # encoded = layers.Dense(7, activation='relu')(encoded)\n",
    "# encoded = layers.Dense(5, activation='relu')(encoded)\n",
    "\n",
    "\n",
    "# decoded = layers.Dense(14, activation='relu')(encoded)\n",
    "# # decoded = layers.Dense(30, activation='relu')(decoded)\n",
    "# decoded = layers.Dense(27, activation='relu')(decoded)\n",
    "# decoded = layers.Dense(54, activation='sigmoid')(decoded)\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "encoding_dim = 5  # 5 floats -> compression of factor 10.8, assuming the input is 54 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(54,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(27, activation='relu')(input_img)\n",
    "encoded = layers.Dense(14, activation='relu')(encoded)\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(14, activation='relu')(encoded)\n",
    "# decoded = layers.Dense(30, activation='relu')(decoded)\n",
    "decoded = layers.Dense(27, activation='relu')(decoded)\n",
    "decoded = layers.Dense(54, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3508 samples, validate on 1504 samples\n",
      "Epoch 1/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.6909 - val_loss: 0.6860\n",
      "Epoch 2/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.6817 - val_loss: 0.6764\n",
      "Epoch 3/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.6714 - val_loss: 0.6651\n",
      "Epoch 4/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.6589 - val_loss: 0.6511\n",
      "Epoch 5/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.6432 - val_loss: 0.6336\n",
      "Epoch 6/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.6238 - val_loss: 0.6121\n",
      "Epoch 7/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.6002 - val_loss: 0.5861\n",
      "Epoch 8/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.5720 - val_loss: 0.5557\n",
      "Epoch 9/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.5399 - val_loss: 0.5219\n",
      "Epoch 10/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.5049 - val_loss: 0.4856\n",
      "Epoch 11/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.4676 - val_loss: 0.4473\n",
      "Epoch 12/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.4289 - val_loss: 0.4086\n",
      "Epoch 13/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.3907 - val_loss: 0.3712\n",
      "Epoch 14/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.3544 - val_loss: 0.3365\n",
      "Epoch 15/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.3213 - val_loss: 0.3053\n",
      "Epoch 16/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.2919 - val_loss: 0.2781\n",
      "Epoch 17/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.2665 - val_loss: 0.2548\n",
      "Epoch 18/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.2450 - val_loss: 0.2352\n",
      "Epoch 19/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.2269 - val_loss: 0.2189\n",
      "Epoch 20/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.2119 - val_loss: 0.2054\n",
      "Epoch 21/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1996 - val_loss: 0.1943\n",
      "Epoch 22/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1894 - val_loss: 0.1852\n",
      "Epoch 23/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1811 - val_loss: 0.1777\n",
      "Epoch 24/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1742 - val_loss: 0.1715\n",
      "Epoch 25/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1685 - val_loss: 0.1665\n",
      "Epoch 26/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1639 - val_loss: 0.1622\n",
      "Epoch 27/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1600 - val_loss: 0.1587\n",
      "Epoch 28/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1567 - val_loss: 0.1558\n",
      "Epoch 29/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1540 - val_loss: 0.1533\n",
      "Epoch 30/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1517 - val_loss: 0.1512\n",
      "Epoch 31/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1497 - val_loss: 0.1495\n",
      "Epoch 32/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1481 - val_loss: 0.1479\n",
      "Epoch 33/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1466 - val_loss: 0.1466\n",
      "Epoch 34/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1454 - val_loss: 0.1455\n",
      "Epoch 35/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1443 - val_loss: 0.1445\n",
      "Epoch 36/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1434 - val_loss: 0.1436\n",
      "Epoch 37/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1426 - val_loss: 0.1429\n",
      "Epoch 38/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1419 - val_loss: 0.1422\n",
      "Epoch 39/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1412 - val_loss: 0.1416\n",
      "Epoch 40/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1407 - val_loss: 0.1411\n",
      "Epoch 41/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1401 - val_loss: 0.1406\n",
      "Epoch 42/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1397 - val_loss: 0.1402\n",
      "Epoch 43/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1393 - val_loss: 0.1398\n",
      "Epoch 44/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1389 - val_loss: 0.1394\n",
      "Epoch 45/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1385 - val_loss: 0.1391\n",
      "Epoch 46/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1382 - val_loss: 0.1388\n",
      "Epoch 47/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1380 - val_loss: 0.1386\n",
      "Epoch 48/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1377 - val_loss: 0.1383\n",
      "Epoch 49/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1374 - val_loss: 0.1381\n",
      "Epoch 50/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1372 - val_loss: 0.1379\n",
      "Epoch 51/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1370 - val_loss: 0.1377\n",
      "Epoch 52/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1368 - val_loss: 0.1375\n",
      "Epoch 53/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1366 - val_loss: 0.1373\n",
      "Epoch 54/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1365 - val_loss: 0.1371\n",
      "Epoch 55/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1363 - val_loss: 0.1370\n",
      "Epoch 56/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1362 - val_loss: 0.1368\n",
      "Epoch 57/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1360 - val_loss: 0.1367\n",
      "Epoch 58/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1359 - val_loss: 0.1366\n",
      "Epoch 59/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1357 - val_loss: 0.1365\n",
      "Epoch 60/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1356 - val_loss: 0.1363\n",
      "Epoch 61/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1355 - val_loss: 0.1362\n",
      "Epoch 62/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1354 - val_loss: 0.1361\n",
      "Epoch 63/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1353 - val_loss: 0.1360\n",
      "Epoch 64/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1352 - val_loss: 0.1359\n",
      "Epoch 65/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1351 - val_loss: 0.1358\n",
      "Epoch 66/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1350 - val_loss: 0.1357\n",
      "Epoch 67/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1349 - val_loss: 0.1356\n",
      "Epoch 68/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1348 - val_loss: 0.1355\n",
      "Epoch 69/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1347 - val_loss: 0.1354\n",
      "Epoch 70/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1346 - val_loss: 0.1353\n",
      "Epoch 71/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1345 - val_loss: 0.1352\n",
      "Epoch 72/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1344 - val_loss: 0.1352\n",
      "Epoch 73/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1343 - val_loss: 0.1351\n",
      "Epoch 74/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1342 - val_loss: 0.1350\n",
      "Epoch 75/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1342 - val_loss: 0.1349\n",
      "Epoch 76/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1341 - val_loss: 0.1348\n",
      "Epoch 77/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1340 - val_loss: 0.1347\n",
      "Epoch 78/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1339 - val_loss: 0.1346\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1338 - val_loss: 0.1346\n",
      "Epoch 80/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1337 - val_loss: 0.1345\n",
      "Epoch 81/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1336 - val_loss: 0.1344\n",
      "Epoch 82/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1336 - val_loss: 0.1343\n",
      "Epoch 83/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1335 - val_loss: 0.1342\n",
      "Epoch 84/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1334 - val_loss: 0.1341\n",
      "Epoch 85/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1333 - val_loss: 0.1340\n",
      "Epoch 86/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1332 - val_loss: 0.1339\n",
      "Epoch 87/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1331 - val_loss: 0.1338\n",
      "Epoch 88/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1330 - val_loss: 0.1338\n",
      "Epoch 89/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1329 - val_loss: 0.1337\n",
      "Epoch 90/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1328 - val_loss: 0.1336\n",
      "Epoch 91/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1327 - val_loss: 0.1335\n",
      "Epoch 92/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1327 - val_loss: 0.1334\n",
      "Epoch 93/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1326 - val_loss: 0.1333\n",
      "Epoch 94/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1325 - val_loss: 0.1332\n",
      "Epoch 95/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1324 - val_loss: 0.1331\n",
      "Epoch 96/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1323 - val_loss: 0.1330\n",
      "Epoch 97/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1322 - val_loss: 0.1329\n",
      "Epoch 98/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1321 - val_loss: 0.1328\n",
      "Epoch 99/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1319 - val_loss: 0.1327\n",
      "Epoch 100/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1318 - val_loss: 0.1326\n",
      "Epoch 101/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1317 - val_loss: 0.1324\n",
      "Epoch 102/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1316 - val_loss: 0.1323\n",
      "Epoch 103/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1315 - val_loss: 0.1322\n",
      "Epoch 104/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1314 - val_loss: 0.1321\n",
      "Epoch 105/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1313 - val_loss: 0.1320\n",
      "Epoch 106/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1312 - val_loss: 0.1319\n",
      "Epoch 107/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1311 - val_loss: 0.1318\n",
      "Epoch 108/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1309 - val_loss: 0.1316\n",
      "Epoch 109/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1308 - val_loss: 0.1315\n",
      "Epoch 110/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1307 - val_loss: 0.1314\n",
      "Epoch 111/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1306 - val_loss: 0.1313\n",
      "Epoch 112/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1304 - val_loss: 0.1312\n",
      "Epoch 113/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1303 - val_loss: 0.1310\n",
      "Epoch 114/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1302 - val_loss: 0.1309\n",
      "Epoch 115/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1301 - val_loss: 0.1308\n",
      "Epoch 116/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1299 - val_loss: 0.1306\n",
      "Epoch 117/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1298 - val_loss: 0.1305\n",
      "Epoch 118/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1297 - val_loss: 0.1304\n",
      "Epoch 119/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1295 - val_loss: 0.1302\n",
      "Epoch 120/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1294 - val_loss: 0.1301\n",
      "Epoch 121/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1293 - val_loss: 0.1300\n",
      "Epoch 122/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1291 - val_loss: 0.1299\n",
      "Epoch 123/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1290 - val_loss: 0.1297\n",
      "Epoch 124/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1289 - val_loss: 0.1296\n",
      "Epoch 125/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1287 - val_loss: 0.1294\n",
      "Epoch 126/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1286 - val_loss: 0.1293\n",
      "Epoch 127/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1285 - val_loss: 0.1292\n",
      "Epoch 128/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1283 - val_loss: 0.1290\n",
      "Epoch 129/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1282 - val_loss: 0.1289\n",
      "Epoch 130/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1280 - val_loss: 0.1288\n",
      "Epoch 131/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1279 - val_loss: 0.1286\n",
      "Epoch 132/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1278 - val_loss: 0.1285\n",
      "Epoch 133/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1276 - val_loss: 0.1283\n",
      "Epoch 134/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1275 - val_loss: 0.1282\n",
      "Epoch 135/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1273 - val_loss: 0.1281\n",
      "Epoch 136/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1272 - val_loss: 0.1279\n",
      "Epoch 137/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1271 - val_loss: 0.1278\n",
      "Epoch 138/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1269 - val_loss: 0.1277\n",
      "Epoch 139/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1268 - val_loss: 0.1275\n",
      "Epoch 140/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1266 - val_loss: 0.1274\n",
      "Epoch 141/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1265 - val_loss: 0.1272\n",
      "Epoch 142/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1264 - val_loss: 0.1271\n",
      "Epoch 143/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1262 - val_loss: 0.1270\n",
      "Epoch 144/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1261 - val_loss: 0.1268\n",
      "Epoch 145/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1260 - val_loss: 0.1267\n",
      "Epoch 146/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1258 - val_loss: 0.1266\n",
      "Epoch 147/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1257 - val_loss: 0.1264\n",
      "Epoch 148/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1255 - val_loss: 0.1263\n",
      "Epoch 149/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1254 - val_loss: 0.1262\n",
      "Epoch 150/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1253 - val_loss: 0.1261\n",
      "Epoch 151/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1251 - val_loss: 0.1259\n",
      "Epoch 152/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1250 - val_loss: 0.1258\n",
      "Epoch 153/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1249 - val_loss: 0.1257\n",
      "Epoch 154/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1248 - val_loss: 0.1255\n",
      "Epoch 155/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1246 - val_loss: 0.1254\n",
      "Epoch 156/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1245 - val_loss: 0.1253\n",
      "Epoch 157/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1244 - val_loss: 0.1252\n",
      "Epoch 158/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1242 - val_loss: 0.1250\n",
      "Epoch 159/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1241 - val_loss: 0.1249\n",
      "Epoch 160/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1240 - val_loss: 0.1248\n",
      "Epoch 161/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1239 - val_loss: 0.1247\n",
      "Epoch 162/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1237 - val_loss: 0.1246\n",
      "Epoch 163/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1236 - val_loss: 0.1244\n",
      "Epoch 164/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1235 - val_loss: 0.1243\n",
      "Epoch 165/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1234 - val_loss: 0.1242\n",
      "Epoch 166/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1233 - val_loss: 0.1241\n",
      "Epoch 167/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1231 - val_loss: 0.1240\n",
      "Epoch 168/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1230 - val_loss: 0.1239\n",
      "Epoch 169/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1229 - val_loss: 0.1238\n",
      "Epoch 170/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1228 - val_loss: 0.1237\n",
      "Epoch 171/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1227 - val_loss: 0.1235\n",
      "Epoch 172/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1226 - val_loss: 0.1234\n",
      "Epoch 173/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1225 - val_loss: 0.1233\n",
      "Epoch 174/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1223 - val_loss: 0.1232\n",
      "Epoch 175/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1222 - val_loss: 0.1231\n",
      "Epoch 176/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1221 - val_loss: 0.1230\n",
      "Epoch 177/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1220 - val_loss: 0.1229\n",
      "Epoch 178/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1219 - val_loss: 0.1228\n",
      "Epoch 179/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1218 - val_loss: 0.1227\n",
      "Epoch 180/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1217 - val_loss: 0.1226\n",
      "Epoch 181/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1216 - val_loss: 0.1225\n",
      "Epoch 182/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1215 - val_loss: 0.1224\n",
      "Epoch 183/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1214 - val_loss: 0.1223\n",
      "Epoch 184/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1213 - val_loss: 0.1222\n",
      "Epoch 185/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1212 - val_loss: 0.1221\n",
      "Epoch 186/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1211 - val_loss: 0.1220\n",
      "Epoch 187/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1210 - val_loss: 0.1219\n",
      "Epoch 188/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1209 - val_loss: 0.1219\n",
      "Epoch 189/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1208 - val_loss: 0.1218\n",
      "Epoch 190/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1207 - val_loss: 0.1217\n",
      "Epoch 191/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1206 - val_loss: 0.1216\n",
      "Epoch 192/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1205 - val_loss: 0.1215\n",
      "Epoch 193/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1204 - val_loss: 0.1214\n",
      "Epoch 194/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1204 - val_loss: 0.1213\n",
      "Epoch 195/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1203 - val_loss: 0.1212\n",
      "Epoch 196/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1202 - val_loss: 0.1212\n",
      "Epoch 197/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1201 - val_loss: 0.1211\n",
      "Epoch 198/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1200 - val_loss: 0.1210\n",
      "Epoch 199/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1199 - val_loss: 0.1209\n",
      "Epoch 200/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1198 - val_loss: 0.1208\n",
      "Epoch 201/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1197 - val_loss: 0.1207\n",
      "Epoch 202/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1196 - val_loss: 0.1206\n",
      "Epoch 203/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1196 - val_loss: 0.1206\n",
      "Epoch 204/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1195 - val_loss: 0.1205\n",
      "Epoch 205/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1194 - val_loss: 0.1204\n",
      "Epoch 206/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1193 - val_loss: 0.1203\n",
      "Epoch 207/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1192 - val_loss: 0.1202\n",
      "Epoch 208/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1191 - val_loss: 0.1202\n",
      "Epoch 209/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1190 - val_loss: 0.1201\n",
      "Epoch 210/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1189 - val_loss: 0.1200\n",
      "Epoch 211/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1189 - val_loss: 0.1199\n",
      "Epoch 212/300\n",
      "3508/3508 [==============================] - ETA: 0s - loss: 0.120 - 0s 5us/step - loss: 0.1188 - val_loss: 0.1198\n",
      "Epoch 213/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1187 - val_loss: 0.1197\n",
      "Epoch 214/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1186 - val_loss: 0.1197\n",
      "Epoch 215/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1185 - val_loss: 0.1196\n",
      "Epoch 216/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1184 - val_loss: 0.1195\n",
      "Epoch 217/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1184 - val_loss: 0.1194\n",
      "Epoch 218/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1183 - val_loss: 0.1193\n",
      "Epoch 219/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1182 - val_loss: 0.1193\n",
      "Epoch 220/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1181 - val_loss: 0.1192\n",
      "Epoch 221/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1180 - val_loss: 0.1191\n",
      "Epoch 222/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1179 - val_loss: 0.1190\n",
      "Epoch 223/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1178 - val_loss: 0.1189\n",
      "Epoch 224/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1178 - val_loss: 0.1189\n",
      "Epoch 225/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1177 - val_loss: 0.1188\n",
      "Epoch 226/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1176 - val_loss: 0.1187\n",
      "Epoch 227/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1175 - val_loss: 0.1186\n",
      "Epoch 228/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1174 - val_loss: 0.1185\n",
      "Epoch 229/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1173 - val_loss: 0.1185\n",
      "Epoch 230/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1172 - val_loss: 0.1184\n",
      "Epoch 231/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1172 - val_loss: 0.1183\n",
      "Epoch 232/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1171 - val_loss: 0.1182\n",
      "Epoch 233/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1170 - val_loss: 0.1181\n",
      "Epoch 234/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1169 - val_loss: 0.1181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1168 - val_loss: 0.1180\n",
      "Epoch 236/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1167 - val_loss: 0.1179\n",
      "Epoch 237/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1166 - val_loss: 0.1178\n",
      "Epoch 238/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1166 - val_loss: 0.1177\n",
      "Epoch 239/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1165 - val_loss: 0.1176\n",
      "Epoch 240/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1164 - val_loss: 0.1176\n",
      "Epoch 241/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1163 - val_loss: 0.1175\n",
      "Epoch 242/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1162 - val_loss: 0.1174\n",
      "Epoch 243/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1161 - val_loss: 0.1173\n",
      "Epoch 244/300\n",
      "3508/3508 [==============================] - ETA: 0s - loss: 0.115 - 0s 4us/step - loss: 0.1160 - val_loss: 0.1172\n",
      "Epoch 245/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1160 - val_loss: 0.1172\n",
      "Epoch 246/300\n",
      "3508/3508 [==============================] - 0s 6us/step - loss: 0.1159 - val_loss: 0.1171\n",
      "Epoch 247/300\n",
      "3508/3508 [==============================] - 0s 5us/step - loss: 0.1158 - val_loss: 0.1170\n",
      "Epoch 248/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1157 - val_loss: 0.1169\n",
      "Epoch 249/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1156 - val_loss: 0.1168\n",
      "Epoch 250/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1155 - val_loss: 0.1168\n",
      "Epoch 251/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1155 - val_loss: 0.1167\n",
      "Epoch 252/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1154 - val_loss: 0.1166\n",
      "Epoch 253/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1153 - val_loss: 0.1165\n",
      "Epoch 254/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1152 - val_loss: 0.1164\n",
      "Epoch 255/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1151 - val_loss: 0.1164\n",
      "Epoch 256/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1150 - val_loss: 0.1163\n",
      "Epoch 257/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1150 - val_loss: 0.1162\n",
      "Epoch 258/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1149 - val_loss: 0.1161\n",
      "Epoch 259/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1148 - val_loss: 0.1160\n",
      "Epoch 260/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1147 - val_loss: 0.1160\n",
      "Epoch 261/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1146 - val_loss: 0.1159\n",
      "Epoch 262/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1145 - val_loss: 0.1158\n",
      "Epoch 263/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1145 - val_loss: 0.1157\n",
      "Epoch 264/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1144 - val_loss: 0.1156\n",
      "Epoch 265/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1143 - val_loss: 0.1156\n",
      "Epoch 266/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1142 - val_loss: 0.1155\n",
      "Epoch 267/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1141 - val_loss: 0.1154\n",
      "Epoch 268/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1141 - val_loss: 0.1153\n",
      "Epoch 269/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1140 - val_loss: 0.1153\n",
      "Epoch 270/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1139 - val_loss: 0.1152\n",
      "Epoch 271/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1138 - val_loss: 0.1151\n",
      "Epoch 272/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1138 - val_loss: 0.1150\n",
      "Epoch 273/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1137 - val_loss: 0.1150\n",
      "Epoch 274/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1136 - val_loss: 0.1149\n",
      "Epoch 275/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1135 - val_loss: 0.1148\n",
      "Epoch 276/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1134 - val_loss: 0.1147\n",
      "Epoch 277/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1134 - val_loss: 0.1147\n",
      "Epoch 278/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1133 - val_loss: 0.1146\n",
      "Epoch 279/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1132 - val_loss: 0.1145\n",
      "Epoch 280/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1132 - val_loss: 0.1145\n",
      "Epoch 281/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1131 - val_loss: 0.1144\n",
      "Epoch 282/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1130 - val_loss: 0.1143\n",
      "Epoch 283/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1129 - val_loss: 0.1143\n",
      "Epoch 284/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1129 - val_loss: 0.1142\n",
      "Epoch 285/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1128 - val_loss: 0.1141\n",
      "Epoch 286/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1127 - val_loss: 0.1141\n",
      "Epoch 287/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1127 - val_loss: 0.1140\n",
      "Epoch 288/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1126 - val_loss: 0.1139\n",
      "Epoch 289/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1125 - val_loss: 0.1139\n",
      "Epoch 290/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1125 - val_loss: 0.1138\n",
      "Epoch 291/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1124 - val_loss: 0.1137\n",
      "Epoch 292/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1124 - val_loss: 0.1137\n",
      "Epoch 293/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1123 - val_loss: 0.1136\n",
      "Epoch 294/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1122 - val_loss: 0.1136\n",
      "Epoch 295/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1122 - val_loss: 0.1135\n",
      "Epoch 296/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1121 - val_loss: 0.1135\n",
      "Epoch 297/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1121 - val_loss: 0.1134\n",
      "Epoch 298/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1120 - val_loss: 0.1133\n",
      "Epoch 299/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1120 - val_loss: 0.1133\n",
      "Epoch 300/300\n",
      "3508/3508 [==============================] - 0s 4us/step - loss: 0.1119 - val_loss: 0.1132\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=300,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(os.path.join(\"/home/kuan-hao/Desktop/loss.png\"), dpi=400)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('len(encoded_imgs): ', 1504)\n",
      "('len(encoded_imgs[0]): ', 5)\n",
      "('len(decoded_imgs[0]): ', 54)\n"
     ]
    }
   ],
   "source": [
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "print(\"len(encoded_imgs): \", len(encoded_imgs))\n",
    "print(\"len(encoded_imgs[0]): \", len(encoded_imgs[0]))\n",
    "print(\"len(decoded_imgs[0]): \", len(decoded_imgs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform input into encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = encoder.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('len(data_encoded): ', 5012)\n",
      "('len(targets): ', 5012)\n"
     ]
    }
   ],
   "source": [
    "print(\"len(data_encoded): \", len(data_encoded))\n",
    "print(\"len(targets): \", len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('     x_train: ', (3508, 5))\n",
      "('     x_test: ', (1504, 5))\n",
      "('     y_train: ', (3508,))\n",
      "('     y_test: ', (1504,))\n"
     ]
    }
   ],
   "source": [
    "# this_targets, preds, probs, precision, recall, fmeasure, auc_pr, auc_roc, curve_pr, curve_roc = clf.cv_eval(data, targets, outDir, k_d_training, folds, train_test_ratio, num_ep, num_frc)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_encoded, targets, test_size=0.3, random_state=1)\n",
    "print(\"     x_train: \", x_train.shape)\n",
    "print(\"     x_test: \", x_test.shape)\n",
    "print(\"     y_train: \", y_train.shape)\n",
    "print(\"     y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95898783, 1.7111433 , 1.6578858 , 3.301536  , 2.1571996 ],\n",
       "       [2.0985763 , 1.7226908 , 1.7012873 , 0.        , 2.3623118 ],\n",
       "       [1.1075162 , 1.0938008 , 1.613379  , 1.5816376 , 1.5809512 ],\n",
       "       ...,\n",
       "       [0.5186173 , 2.2352295 , 1.7778525 , 2.1587675 , 1.0290148 ],\n",
       "       [1.4964948 , 1.4587287 , 0.92274106, 0.670108  , 2.1766384 ],\n",
       "       [1.5855032 , 1.2713962 , 1.3904507 , 2.7344291 , 1.7089255 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Processing data...\"\n",
    "self.clf = CNN_raw_ef_model(num_ep, num_frc)\n",
    "self.clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "self.fit(x_train, y_train)\n",
    "probs = self.predict_proba(x_test)\n",
    "preds = self.predict(x_test)\n",
    "this_targets = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start benchmarking\n",
      "('&& self.scores.shape[0]: ', 5012)\n",
      "('&& pos_count: ', 244)\n",
      "('&& neg_count: ', 244)\n",
      "('&& self.scores.shape[0]: ', 5012)\n",
      "('&& pos_count_all: ', 244)\n",
      "('&& neg_count_all: ', 4768)\n",
      "('&& unsure_count_all: ', 0)\n",
      "('data: ', (488, 2, 27))\n",
      "('data.shape: ', (488, 2, 27))\n",
      "('targets.shape: ', (488,))\n",
      "('data_all: ', (5012, 2, 27))\n",
      "('data_all.shape: ', (5012, 2, 27))\n",
      "('targets_all.shape: ', (5012,))\n",
      "('data: ', (488, 2, 27))\n",
      "('data.shape: ', (488, 2, 27, 1))\n",
      "[[[0.        ]\n",
      "  [0.03388044]\n",
      "  [0.05531865]\n",
      "  [0.05226366]\n",
      "  [0.04394045]\n",
      "  [0.09155187]\n",
      "  [0.02190318]\n",
      "  [0.04774131]\n",
      "  [0.03144744]\n",
      "  [0.07235891]\n",
      "  [0.06894582]\n",
      "  [0.31839759]\n",
      "  [0.05387115]\n",
      "  [0.07679509]\n",
      "  [0.01711927]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00577184]\n",
      "  [0.00869332]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.05082859]\n",
      "  [0.02380308]\n",
      "  [0.05561676]\n",
      "  [0.11654707]\n",
      "  [0.43502791]\n",
      "  [0.27920826]\n",
      "  [0.03896832]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]]\n",
      "('targets.shape: ', (488,))\n",
      "[0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0\n",
      " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1\n",
      " 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1\n",
      " 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
      " 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1\n",
      " 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0\n",
      " 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0\n",
      " 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
      " 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1\n",
      " 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
      " 0 1 0 0 1 1 1]\n",
      "Model Evaluation ~!\n",
      "    Training / Testing Ratio : 0.3/0.7\n",
      "('     x_train: ', (341, 2, 27, 1))\n",
      "('     x_test: ', (147, 2, 27, 1))\n",
      "('     y_train: ', (341,))\n",
      "('     y_test: ', (147,))\n",
      "Processing data...\n",
      "Epoch 1/200\n",
      "341/341 [==============================] - 0s 52us/step - loss: 0.6933 - acc: 0.4692\n",
      "Epoch 2/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.6931 - acc: 0.5103\n",
      "Epoch 3/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.6929 - acc: 0.5367\n",
      "Epoch 4/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.6925 - acc: 0.5103\n",
      "Epoch 5/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.6929 - acc: 0.5191\n",
      "Epoch 6/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.6920 - acc: 0.5572\n",
      "Epoch 7/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.6900 - acc: 0.5513\n",
      "Epoch 8/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.6915 - acc: 0.5396\n",
      "Epoch 9/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.6879 - acc: 0.5689\n",
      "Epoch 10/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.6848 - acc: 0.6305\n",
      "Epoch 11/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.6761 - acc: 0.6217\n",
      "Epoch 12/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.6721 - acc: 0.6422\n",
      "Epoch 13/200\n",
      "341/341 [==============================] - 0s 50us/step - loss: 0.6645 - acc: 0.6628\n",
      "Epoch 14/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.6503 - acc: 0.6745\n",
      "Epoch 15/200\n",
      "341/341 [==============================] - 0s 51us/step - loss: 0.6317 - acc: 0.6745\n",
      "Epoch 16/200\n",
      "341/341 [==============================] - 0s 51us/step - loss: 0.6123 - acc: 0.7185\n",
      "Epoch 17/200\n",
      "341/341 [==============================] - 0s 62us/step - loss: 0.6172 - acc: 0.7155\n",
      "Epoch 18/200\n",
      "341/341 [==============================] - 0s 62us/step - loss: 0.5908 - acc: 0.7390\n",
      "Epoch 19/200\n",
      "341/341 [==============================] - 0s 63us/step - loss: 0.5936 - acc: 0.7419\n",
      "Epoch 20/200\n",
      "341/341 [==============================] - 0s 59us/step - loss: 0.5613 - acc: 0.7449\n",
      "Epoch 21/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.5575 - acc: 0.7683\n",
      "Epoch 22/200\n",
      "341/341 [==============================] - 0s 50us/step - loss: 0.5444 - acc: 0.7713\n",
      "Epoch 23/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.5458 - acc: 0.7801\n",
      "Epoch 24/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.5134 - acc: 0.7830\n",
      "Epoch 25/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.5347 - acc: 0.7390\n",
      "Epoch 26/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.5150 - acc: 0.7918\n",
      "Epoch 27/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.5304 - acc: 0.7713\n",
      "Epoch 28/200\n",
      "341/341 [==============================] - 0s 62us/step - loss: 0.5150 - acc: 0.7918\n",
      "Epoch 29/200\n",
      "341/341 [==============================] - 0s 62us/step - loss: 0.4777 - acc: 0.8094\n",
      "Epoch 30/200\n",
      "341/341 [==============================] - 0s 60us/step - loss: 0.4647 - acc: 0.8182\n",
      "Epoch 31/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.4552 - acc: 0.8416\n",
      "Epoch 32/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.4778 - acc: 0.8211\n",
      "Epoch 33/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.4529 - acc: 0.8065\n",
      "Epoch 34/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.4741 - acc: 0.8065\n",
      "Epoch 35/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.4230 - acc: 0.8680\n",
      "Epoch 36/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.4375 - acc: 0.8123\n",
      "Epoch 37/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.4497 - acc: 0.8299\n",
      "Epoch 38/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.4271 - acc: 0.8211\n",
      "Epoch 39/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.4380 - acc: 0.8299\n",
      "Epoch 40/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.4250 - acc: 0.8240\n",
      "Epoch 41/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.4073 - acc: 0.8475\n",
      "Epoch 42/200\n",
      "341/341 [==============================] - 0s 50us/step - loss: 0.3662 - acc: 0.8622\n",
      "Epoch 43/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.4074 - acc: 0.8446\n",
      "Epoch 44/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.3774 - acc: 0.8563\n",
      "Epoch 45/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.3807 - acc: 0.8563\n",
      "Epoch 46/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.3774 - acc: 0.8416\n",
      "Epoch 47/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.3482 - acc: 0.8622\n",
      "Epoch 48/200\n",
      "341/341 [==============================] - 0s 58us/step - loss: 0.3733 - acc: 0.8622\n",
      "Epoch 49/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.3650 - acc: 0.8504\n",
      "Epoch 50/200\n",
      "341/341 [==============================] - 0s 49us/step - loss: 0.3845 - acc: 0.8358\n",
      "Epoch 51/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.3473 - acc: 0.8827\n",
      "Epoch 52/200\n",
      "341/341 [==============================] - 0s 48us/step - loss: 0.3623 - acc: 0.8680\n",
      "Epoch 53/200\n",
      "341/341 [==============================] - 0s 50us/step - loss: 0.3442 - acc: 0.8592\n",
      "Epoch 54/200\n",
      "341/341 [==============================] - ETA: 0s - loss: 0.2877 - acc: 0.906 - 0s 52us/step - loss: 0.3384 - acc: 0.8710\n",
      "Epoch 55/200\n",
      "341/341 [==============================] - 0s 56us/step - loss: 0.3472 - acc: 0.8563\n",
      "Epoch 56/200\n",
      "341/341 [==============================] - 0s 61us/step - loss: 0.3454 - acc: 0.8739\n",
      "Epoch 57/200\n",
      "341/341 [==============================] - 0s 70us/step - loss: 0.3358 - acc: 0.8739\n",
      "Epoch 58/200\n",
      "341/341 [==============================] - 0s 84us/step - loss: 0.3574 - acc: 0.8622\n",
      "Epoch 59/200\n",
      "341/341 [==============================] - 0s 94us/step - loss: 0.3250 - acc: 0.8739\n",
      "Epoch 60/200\n",
      "341/341 [==============================] - 0s 106us/step - loss: 0.3437 - acc: 0.8680\n",
      "Epoch 61/200\n",
      "341/341 [==============================] - 0s 113us/step - loss: 0.3413 - acc: 0.8680\n",
      "Epoch 62/200\n",
      "341/341 [==============================] - 0s 119us/step - loss: 0.3360 - acc: 0.8856\n",
      "Epoch 63/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.3333 - acc: 0.8622\n",
      "Epoch 64/200\n",
      "341/341 [==============================] - 0s 132us/step - loss: 0.3467 - acc: 0.8622\n",
      "Epoch 65/200\n",
      "341/341 [==============================] - 0s 140us/step - loss: 0.3246 - acc: 0.8739\n",
      "Epoch 66/200\n",
      "341/341 [==============================] - 0s 145us/step - loss: 0.3059 - acc: 0.8856\n",
      "Epoch 67/200\n",
      "341/341 [==============================] - 0s 148us/step - loss: 0.2921 - acc: 0.8886\n",
      "Epoch 68/200\n",
      "341/341 [==============================] - 0s 163us/step - loss: 0.3049 - acc: 0.8739\n",
      "Epoch 69/200\n",
      "341/341 [==============================] - 0s 150us/step - loss: 0.3101 - acc: 0.8915\n",
      "Epoch 70/200\n",
      "341/341 [==============================] - 0s 151us/step - loss: 0.3327 - acc: 0.8710\n",
      "Epoch 71/200\n",
      "341/341 [==============================] - 0s 149us/step - loss: 0.2816 - acc: 0.9003\n",
      "Epoch 72/200\n",
      "341/341 [==============================] - 0s 147us/step - loss: 0.2858 - acc: 0.8915\n",
      "Epoch 73/200\n",
      "341/341 [==============================] - 0s 142us/step - loss: 0.2744 - acc: 0.8944\n",
      "Epoch 74/200\n",
      "341/341 [==============================] - 0s 137us/step - loss: 0.2820 - acc: 0.9032\n",
      "Epoch 75/200\n",
      "341/341 [==============================] - 0s 133us/step - loss: 0.2691 - acc: 0.8915\n",
      "Epoch 76/200\n",
      "341/341 [==============================] - 0s 133us/step - loss: 0.3008 - acc: 0.8768\n",
      "Epoch 77/200\n",
      "341/341 [==============================] - 0s 130us/step - loss: 0.2822 - acc: 0.9003\n",
      "Epoch 78/200\n",
      "341/341 [==============================] - 0s 128us/step - loss: 0.2600 - acc: 0.9032\n",
      "Epoch 79/200\n",
      "341/341 [==============================] - 0s 127us/step - loss: 0.3179 - acc: 0.8739\n",
      "Epoch 80/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.2495 - acc: 0.9150\n",
      "Epoch 81/200\n",
      "341/341 [==============================] - 0s 123us/step - loss: 0.2907 - acc: 0.8798\n",
      "Epoch 82/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.2773 - acc: 0.9003\n",
      "Epoch 83/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.2646 - acc: 0.9062\n",
      "Epoch 84/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.2495 - acc: 0.9032\n",
      "Epoch 85/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.2397 - acc: 0.9032\n",
      "Epoch 86/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.2715 - acc: 0.9003\n",
      "Epoch 87/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.2823 - acc: 0.8915\n",
      "Epoch 88/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.2555 - acc: 0.9120\n",
      "Epoch 89/200\n",
      "341/341 [==============================] - 0s 130us/step - loss: 0.2211 - acc: 0.9091\n",
      "Epoch 90/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.2570 - acc: 0.9032\n",
      "Epoch 91/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.2457 - acc: 0.9150\n",
      "Epoch 92/200\n",
      "341/341 [==============================] - 0s 123us/step - loss: 0.2500 - acc: 0.8944\n",
      "Epoch 93/200\n",
      "341/341 [==============================] - 0s 123us/step - loss: 0.2435 - acc: 0.8974\n",
      "Epoch 94/200\n",
      "341/341 [==============================] - 0s 123us/step - loss: 0.2202 - acc: 0.9238\n",
      "Epoch 95/200\n",
      "341/341 [==============================] - 0s 122us/step - loss: 0.2179 - acc: 0.9208\n",
      "Epoch 96/200\n",
      "341/341 [==============================] - 0s 122us/step - loss: 0.2090 - acc: 0.9091\n",
      "Epoch 97/200\n",
      "341/341 [==============================] - 0s 121us/step - loss: 0.2345 - acc: 0.9091\n",
      "Epoch 98/200\n",
      "341/341 [==============================] - 0s 120us/step - loss: 0.2184 - acc: 0.9179\n",
      "Epoch 99/200\n",
      "341/341 [==============================] - 0s 120us/step - loss: 0.2750 - acc: 0.8974\n",
      "Epoch 100/200\n",
      "341/341 [==============================] - 0s 119us/step - loss: 0.2368 - acc: 0.8974\n",
      "Epoch 101/200\n",
      "341/341 [==============================] - 0s 120us/step - loss: 0.2344 - acc: 0.9120\n",
      "Epoch 102/200\n",
      "341/341 [==============================] - 0s 119us/step - loss: 0.2310 - acc: 0.8944\n",
      "Epoch 103/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.2284 - acc: 0.9267\n",
      "Epoch 104/200\n",
      "341/341 [==============================] - 0s 122us/step - loss: 0.2138 - acc: 0.9150\n",
      "Epoch 105/200\n",
      "341/341 [==============================] - 0s 120us/step - loss: 0.2193 - acc: 0.9032\n",
      "Epoch 106/200\n",
      "341/341 [==============================] - 0s 120us/step - loss: 0.2285 - acc: 0.8974\n",
      "Epoch 107/200\n",
      "341/341 [==============================] - 0s 127us/step - loss: 0.2361 - acc: 0.9091\n",
      "Epoch 108/200\n",
      "341/341 [==============================] - 0s 121us/step - loss: 0.2290 - acc: 0.9003\n",
      "Epoch 109/200\n",
      "341/341 [==============================] - 0s 122us/step - loss: 0.2253 - acc: 0.9091\n",
      "Epoch 110/200\n",
      "341/341 [==============================] - 0s 132us/step - loss: 0.2145 - acc: 0.9032\n",
      "Epoch 111/200\n",
      "341/341 [==============================] - 0s 122us/step - loss: 0.1835 - acc: 0.9208\n",
      "Epoch 112/200\n",
      "341/341 [==============================] - 0s 123us/step - loss: 0.1990 - acc: 0.9062\n",
      "Epoch 113/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.2070 - acc: 0.9120\n",
      "Epoch 114/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.2264 - acc: 0.8974\n",
      "Epoch 115/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1974 - acc: 0.9120\n",
      "Epoch 116/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.2150 - acc: 0.9032\n",
      "Epoch 117/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1972 - acc: 0.9238\n",
      "Epoch 118/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.2153 - acc: 0.9120\n",
      "Epoch 119/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1472 - acc: 0.9384\n",
      "Epoch 120/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.2138 - acc: 0.9150\n",
      "Epoch 121/200\n",
      "341/341 [==============================] - 0s 136us/step - loss: 0.1352 - acc: 0.9531\n",
      "Epoch 122/200\n",
      "341/341 [==============================] - 0s 127us/step - loss: 0.1934 - acc: 0.9120\n",
      "Epoch 123/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1583 - acc: 0.9267\n",
      "Epoch 124/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1916 - acc: 0.9208\n",
      "Epoch 125/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1594 - acc: 0.9355\n",
      "Epoch 126/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.2065 - acc: 0.8974\n",
      "Epoch 127/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1960 - acc: 0.9238\n",
      "Epoch 128/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.2292 - acc: 0.9120\n",
      "Epoch 129/200\n",
      "341/341 [==============================] - 0s 123us/step - loss: 0.1908 - acc: 0.9150\n",
      "Epoch 130/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.1909 - acc: 0.9091\n",
      "Epoch 131/200\n",
      "341/341 [==============================] - 0s 122us/step - loss: 0.1844 - acc: 0.9091\n",
      "Epoch 132/200\n",
      "341/341 [==============================] - 0s 123us/step - loss: 0.1967 - acc: 0.9062\n",
      "Epoch 133/200\n",
      "341/341 [==============================] - 0s 123us/step - loss: 0.2011 - acc: 0.8944\n",
      "Epoch 134/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.1969 - acc: 0.9238\n",
      "Epoch 135/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.1750 - acc: 0.9179\n",
      "Epoch 136/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.1563 - acc: 0.9413\n",
      "Epoch 137/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.1993 - acc: 0.9208\n",
      "Epoch 138/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.2056 - acc: 0.9120\n",
      "Epoch 139/200\n",
      "341/341 [==============================] - 0s 123us/step - loss: 0.1711 - acc: 0.9208\n",
      "Epoch 140/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 0s 123us/step - loss: 0.2064 - acc: 0.9091\n",
      "Epoch 141/200\n",
      "341/341 [==============================] - 0s 124us/step - loss: 0.1701 - acc: 0.9267\n",
      "Epoch 142/200\n",
      "341/341 [==============================] - 0s 137us/step - loss: 0.2118 - acc: 0.9003\n",
      "Epoch 143/200\n",
      "341/341 [==============================] - 0s 135us/step - loss: 0.1606 - acc: 0.9413\n",
      "Epoch 144/200\n",
      "341/341 [==============================] - 0s 135us/step - loss: 0.1763 - acc: 0.9296\n",
      "Epoch 145/200\n",
      "341/341 [==============================] - 0s 133us/step - loss: 0.1672 - acc: 0.9150\n",
      "Epoch 146/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1698 - acc: 0.9267\n",
      "Epoch 147/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1734 - acc: 0.9179\n",
      "Epoch 148/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1566 - acc: 0.9296\n",
      "Epoch 149/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1802 - acc: 0.9091\n",
      "Epoch 150/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1798 - acc: 0.9208\n",
      "Epoch 151/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1943 - acc: 0.9179\n",
      "Epoch 152/200\n",
      "341/341 [==============================] - 0s 127us/step - loss: 0.1897 - acc: 0.9179\n",
      "Epoch 153/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1629 - acc: 0.9120\n",
      "Epoch 154/200\n",
      "341/341 [==============================] - 0s 127us/step - loss: 0.1573 - acc: 0.9355\n",
      "Epoch 155/200\n",
      "341/341 [==============================] - 0s 127us/step - loss: 0.1635 - acc: 0.9091\n",
      "Epoch 156/200\n",
      "341/341 [==============================] - 0s 128us/step - loss: 0.1515 - acc: 0.9413\n",
      "Epoch 157/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1612 - acc: 0.9267\n",
      "Epoch 158/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1650 - acc: 0.9296\n",
      "Epoch 159/200\n",
      "341/341 [==============================] - 0s 142us/step - loss: 0.2016 - acc: 0.9238\n",
      "Epoch 160/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1893 - acc: 0.9150\n",
      "Epoch 161/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1729 - acc: 0.9296\n",
      "Epoch 162/200\n",
      "341/341 [==============================] - 0s 135us/step - loss: 0.1562 - acc: 0.9267\n",
      "Epoch 163/200\n",
      "341/341 [==============================] - 0s 136us/step - loss: 0.1427 - acc: 0.9267\n",
      "Epoch 164/200\n",
      "341/341 [==============================] - 0s 136us/step - loss: 0.1631 - acc: 0.9355\n",
      "Epoch 165/200\n",
      "341/341 [==============================] - 0s 135us/step - loss: 0.1745 - acc: 0.9326\n",
      "Epoch 166/200\n",
      "341/341 [==============================] - 0s 134us/step - loss: 0.1431 - acc: 0.9384\n",
      "Epoch 167/200\n",
      "341/341 [==============================] - 0s 139us/step - loss: 0.1827 - acc: 0.9179\n",
      "Epoch 168/200\n",
      "341/341 [==============================] - 0s 138us/step - loss: 0.1536 - acc: 0.9355\n",
      "Epoch 169/200\n",
      "341/341 [==============================] - 0s 134us/step - loss: 0.1340 - acc: 0.9443\n",
      "Epoch 170/200\n",
      "341/341 [==============================] - 0s 135us/step - loss: 0.1616 - acc: 0.9208\n",
      "Epoch 171/200\n",
      "341/341 [==============================] - 0s 135us/step - loss: 0.1564 - acc: 0.9355\n",
      "Epoch 172/200\n",
      "341/341 [==============================] - 0s 135us/step - loss: 0.1491 - acc: 0.9326\n",
      "Epoch 173/200\n",
      "341/341 [==============================] - 0s 138us/step - loss: 0.1771 - acc: 0.9179\n",
      "Epoch 174/200\n",
      "341/341 [==============================] - 0s 135us/step - loss: 0.2017 - acc: 0.9091\n",
      "Epoch 175/200\n",
      "341/341 [==============================] - 0s 135us/step - loss: 0.1705 - acc: 0.9179\n",
      "Epoch 176/200\n",
      "341/341 [==============================] - 0s 139us/step - loss: 0.1665 - acc: 0.9384\n",
      "Epoch 177/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1528 - acc: 0.9355\n",
      "Epoch 178/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1457 - acc: 0.9267\n",
      "Epoch 179/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1604 - acc: 0.9296\n",
      "Epoch 180/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1541 - acc: 0.9326\n",
      "Epoch 181/200\n",
      "341/341 [==============================] - 0s 127us/step - loss: 0.1402 - acc: 0.9472\n",
      "Epoch 182/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1638 - acc: 0.9238\n",
      "Epoch 183/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1237 - acc: 0.9413\n",
      "Epoch 184/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1515 - acc: 0.9120\n",
      "Epoch 185/200\n",
      "341/341 [==============================] - 0s 127us/step - loss: 0.1378 - acc: 0.9355\n",
      "Epoch 186/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1266 - acc: 0.9384\n",
      "Epoch 187/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1540 - acc: 0.9238\n",
      "Epoch 188/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1344 - acc: 0.9238\n",
      "Epoch 189/200\n",
      "341/341 [==============================] - 0s 125us/step - loss: 0.1249 - acc: 0.9443\n",
      "Epoch 190/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1420 - acc: 0.9384\n",
      "Epoch 191/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1312 - acc: 0.9384\n",
      "Epoch 192/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1332 - acc: 0.9384\n",
      "Epoch 193/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1530 - acc: 0.9355\n",
      "Epoch 194/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1304 - acc: 0.9443\n",
      "Epoch 195/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1466 - acc: 0.9355\n",
      "Epoch 196/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1277 - acc: 0.9384\n",
      "Epoch 197/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1397 - acc: 0.9355\n",
      "Epoch 198/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1523 - acc: 0.9355\n",
      "Epoch 199/200\n",
      "341/341 [==============================] - 0s 127us/step - loss: 0.1639 - acc: 0.9267\n",
      "Epoch 200/200\n",
      "341/341 [==============================] - 0s 126us/step - loss: 0.1471 - acc: 0.9267\n",
      "('outDir: ', '/home/kuan-hao/EPPC/output/TEST//TEST/all_pos_neg_testing')\n",
      "     ** Plot confusion_matrix: \n",
      "     ** Plot normalized confusion_matrix: \n",
      "     ** Plot curve_roc: \n",
      "     ** Plot curve_pr: \n"
     ]
    }
   ],
   "source": [
    "# ##########################################\n",
    "# ## This part is \"EVALUATION\"\n",
    "# ##########################################\n",
    "# print \"Start benchmarking\"\n",
    "\n",
    "# if mode == \"EXP\":\n",
    "#     utils.cv_bench_clf(scoreCalc, clf, gs, output_dir, format=\"pdf\", verbose=True, k_d_training = K_D_TRAIN, folds = FOLD_NUM, train_test_ratio = TRAIN_TEST_RATIO, num_ep = NUM_EP, num_frc = NUM_FRC)\n",
    "\n",
    "# if mode == \"COMB\":\n",
    "#     tmp_sc = copy.deepcopy(scoreCalc)\n",
    "#     tmp_sc.add_fun_anno(functionalData)\n",
    "#     utils.cv_bench_clf(tmp_sc, clf, gs, output_dir, format=\"pdf\", verbose=True, k_d_training = K_D_TRAIN, folds = FOLD_NUM, train_test_ratio = TRAIN_TEST_RATIO, num_ep = NUM_EP, num_frc = NUM_FRC)\n",
    "\n",
    "# if mode == \"FA\":\n",
    "#     utils.cv_bench_clf(functionalData, clf, gs, output_dir, format=\"pdf\", verbose=True, k_d_training = K_D_TRAIN, folds = FOLD_NUM, train_test_ratio = TRAIN_TEST_RATIO, num_ep = NUM_EP, num_frc = NUM_FRC)\n",
    "# ##########################################\n",
    "# ## End of \"EVALUATION\"\n",
    "# ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('&& self.scores.shape[0]: ', 5012)\n",
      "('&& pos_count: ', 244)\n",
      "('&& neg_count: ', 244)\n",
      "('&& self.scores.shape[0]: ', 5012)\n",
      "('&& pos_count_all: ', 244)\n",
      "('&& neg_count_all: ', 4768)\n",
      "('&& unsure_count_all: ', 0)\n",
      "('data_train: ', (488, 2, 27))\n",
      "('data_train.shape: ', (488, 2, 27, 1))\n",
      "('data_train: ', (5012, 2, 27))\n",
      "('data_all.shape: ', (5012, 2, 27, 1))\n",
      "Final Model Creation!\n",
      "Processing data...\n",
      "Epoch 1/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6938 - acc: 0.4775\n",
      "Epoch 2/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6926 - acc: 0.5041\n",
      "Epoch 3/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6918 - acc: 0.5102\n",
      "Epoch 4/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6892 - acc: 0.5205\n",
      "Epoch 5/200\n",
      "488/488 [==============================] - 0s 46us/step - loss: 0.6855 - acc: 0.5512\n",
      "Epoch 6/200\n",
      "488/488 [==============================] - 0s 46us/step - loss: 0.6829 - acc: 0.5963\n",
      "Epoch 7/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6776 - acc: 0.5738\n",
      "Epoch 8/200\n",
      "488/488 [==============================] - 0s 46us/step - loss: 0.6699 - acc: 0.6865\n",
      "Epoch 9/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6637 - acc: 0.6598\n",
      "Epoch 10/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6574 - acc: 0.6803\n",
      "Epoch 11/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6486 - acc: 0.6926\n",
      "Epoch 12/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6314 - acc: 0.7070\n",
      "Epoch 13/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.6002 - acc: 0.7439\n",
      "Epoch 14/200\n",
      "488/488 [==============================] - 0s 46us/step - loss: 0.6166 - acc: 0.7398\n",
      "Epoch 15/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.5829 - acc: 0.7684\n",
      "Epoch 16/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.5791 - acc: 0.7602\n",
      "Epoch 17/200\n",
      "488/488 [==============================] - 0s 48us/step - loss: 0.5565 - acc: 0.7787\n",
      "Epoch 18/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.5562 - acc: 0.7684\n",
      "Epoch 19/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.5496 - acc: 0.7766\n",
      "Epoch 20/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.5502 - acc: 0.7643\n",
      "Epoch 21/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.5393 - acc: 0.7930\n",
      "Epoch 22/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.5153 - acc: 0.7787\n",
      "Epoch 23/200\n",
      "488/488 [==============================] - 0s 49us/step - loss: 0.5392 - acc: 0.7705\n",
      "Epoch 24/200\n",
      "488/488 [==============================] - 0s 46us/step - loss: 0.5030 - acc: 0.7910\n",
      "Epoch 25/200\n",
      "488/488 [==============================] - 0s 46us/step - loss: 0.5018 - acc: 0.7951\n",
      "Epoch 26/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.4912 - acc: 0.7848\n",
      "Epoch 27/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.4764 - acc: 0.8053\n",
      "Epoch 28/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.4885 - acc: 0.7951\n",
      "Epoch 29/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.4815 - acc: 0.7971\n",
      "Epoch 30/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.4716 - acc: 0.8012\n",
      "Epoch 31/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.4788 - acc: 0.7848\n",
      "Epoch 32/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.4911 - acc: 0.7807\n",
      "Epoch 33/200\n",
      "488/488 [==============================] - 0s 48us/step - loss: 0.4633 - acc: 0.8074\n",
      "Epoch 34/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.4712 - acc: 0.8033\n",
      "Epoch 35/200\n",
      "488/488 [==============================] - 0s 47us/step - loss: 0.4200 - acc: 0.8443\n",
      "Epoch 36/200\n",
      "488/488 [==============================] - 0s 48us/step - loss: 0.4490 - acc: 0.8197\n",
      "Epoch 37/200\n",
      "488/488 [==============================] - 0s 48us/step - loss: 0.4255 - acc: 0.8402\n",
      "Epoch 38/200\n",
      "488/488 [==============================] - 0s 51us/step - loss: 0.4140 - acc: 0.8258\n",
      "Epoch 39/200\n",
      "488/488 [==============================] - 0s 60us/step - loss: 0.4499 - acc: 0.8197\n",
      "Epoch 40/200\n",
      "488/488 [==============================] - 0s 71us/step - loss: 0.4210 - acc: 0.8381\n",
      "Epoch 41/200\n",
      "488/488 [==============================] - 0s 91us/step - loss: 0.4617 - acc: 0.8012\n",
      "Epoch 42/200\n",
      "488/488 [==============================] - 0s 106us/step - loss: 0.4224 - acc: 0.8279\n",
      "Epoch 43/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.4505 - acc: 0.8197\n",
      "Epoch 44/200\n",
      "488/488 [==============================] - 0s 127us/step - loss: 0.4301 - acc: 0.8340\n",
      "Epoch 45/200\n",
      "488/488 [==============================] - 0s 132us/step - loss: 0.4189 - acc: 0.8422\n",
      "Epoch 46/200\n",
      "488/488 [==============================] - 0s 134us/step - loss: 0.3957 - acc: 0.8545\n",
      "Epoch 47/200\n",
      "488/488 [==============================] - 0s 136us/step - loss: 0.3906 - acc: 0.8299\n",
      "Epoch 48/200\n",
      "488/488 [==============================] - 0s 136us/step - loss: 0.4138 - acc: 0.8402\n",
      "Epoch 49/200\n",
      "488/488 [==============================] - 0s 134us/step - loss: 0.3916 - acc: 0.8422\n",
      "Epoch 50/200\n",
      "488/488 [==============================] - 0s 128us/step - loss: 0.4041 - acc: 0.8422\n",
      "Epoch 51/200\n",
      "488/488 [==============================] - 0s 117us/step - loss: 0.3891 - acc: 0.8627\n",
      "Epoch 52/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.4090 - acc: 0.8463\n",
      "Epoch 53/200\n",
      "488/488 [==============================] - 0s 109us/step - loss: 0.4151 - acc: 0.8443\n",
      "Epoch 54/200\n",
      "488/488 [==============================] - 0s 108us/step - loss: 0.4022 - acc: 0.8627\n",
      "Epoch 55/200\n",
      "488/488 [==============================] - 0s 108us/step - loss: 0.3818 - acc: 0.8443\n",
      "Epoch 56/200\n",
      "488/488 [==============================] - 0s 109us/step - loss: 0.3734 - acc: 0.8586\n",
      "Epoch 57/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.3775 - acc: 0.8566\n",
      "Epoch 58/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3880 - acc: 0.8484\n",
      "Epoch 59/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3466 - acc: 0.8791\n",
      "Epoch 60/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3482 - acc: 0.8791\n",
      "Epoch 61/200\n",
      "488/488 [==============================] - 0s 117us/step - loss: 0.3598 - acc: 0.8627\n",
      "Epoch 62/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3801 - acc: 0.8607\n",
      "Epoch 63/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3778 - acc: 0.8627\n",
      "Epoch 64/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3524 - acc: 0.8648\n",
      "Epoch 65/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3448 - acc: 0.8627\n",
      "Epoch 66/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.3677 - acc: 0.8607\n",
      "Epoch 67/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3858 - acc: 0.8566\n",
      "Epoch 68/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.3641 - acc: 0.8627\n",
      "Epoch 69/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3563 - acc: 0.8689\n",
      "Epoch 70/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3491 - acc: 0.8791\n",
      "Epoch 71/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3604 - acc: 0.8689\n",
      "Epoch 72/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3588 - acc: 0.8689\n",
      "Epoch 73/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3288 - acc: 0.8852\n",
      "Epoch 74/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3248 - acc: 0.8873\n",
      "Epoch 75/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3271 - acc: 0.8873\n",
      "Epoch 76/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.3667 - acc: 0.8730\n",
      "Epoch 77/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3144 - acc: 0.8832\n",
      "Epoch 78/200\n",
      "488/488 [==============================] - 0s 124us/step - loss: 0.3571 - acc: 0.8709\n",
      "Epoch 79/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3797 - acc: 0.8607\n",
      "Epoch 80/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3380 - acc: 0.8750\n",
      "Epoch 81/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3354 - acc: 0.8791\n",
      "Epoch 82/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3114 - acc: 0.8914\n",
      "Epoch 83/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3166 - acc: 0.8852\n",
      "Epoch 84/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3240 - acc: 0.8730\n",
      "Epoch 85/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3039 - acc: 0.8975\n",
      "Epoch 86/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3488 - acc: 0.8689\n",
      "Epoch 87/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3688 - acc: 0.8668\n",
      "Epoch 88/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3198 - acc: 0.8873\n",
      "Epoch 89/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3275 - acc: 0.8791\n",
      "Epoch 90/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3161 - acc: 0.8893\n",
      "Epoch 91/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3470 - acc: 0.8791\n",
      "Epoch 92/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3439 - acc: 0.8852\n",
      "Epoch 93/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3132 - acc: 0.8893\n",
      "Epoch 94/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3178 - acc: 0.8893\n",
      "Epoch 95/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3069 - acc: 0.9078\n",
      "Epoch 96/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3519 - acc: 0.8730\n",
      "Epoch 97/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3375 - acc: 0.8750\n",
      "Epoch 98/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3720 - acc: 0.8668\n",
      "Epoch 99/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3365 - acc: 0.8770\n",
      "Epoch 100/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3172 - acc: 0.8914\n",
      "Epoch 101/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3119 - acc: 0.8975\n",
      "Epoch 102/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3158 - acc: 0.8934\n",
      "Epoch 103/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.3364 - acc: 0.8791\n",
      "Epoch 104/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.3125 - acc: 0.8975\n",
      "Epoch 105/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.3028 - acc: 0.9037\n",
      "Epoch 106/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3562 - acc: 0.8791\n",
      "Epoch 107/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.3130 - acc: 0.8832\n",
      "Epoch 108/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.2762 - acc: 0.9078\n",
      "Epoch 109/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.3085 - acc: 0.8975\n",
      "Epoch 110/200\n",
      "488/488 [==============================] - 0s 109us/step - loss: 0.2960 - acc: 0.9037\n",
      "Epoch 111/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.3067 - acc: 0.9016\n",
      "Epoch 112/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.3303 - acc: 0.8893\n",
      "Epoch 113/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.2879 - acc: 0.9057\n",
      "Epoch 114/200\n",
      "488/488 [==============================] - 0s 109us/step - loss: 0.3050 - acc: 0.8934\n",
      "Epoch 115/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.2781 - acc: 0.9037\n",
      "Epoch 116/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.3243 - acc: 0.8873\n",
      "Epoch 117/200\n",
      "488/488 [==============================] - 0s 110us/step - loss: 0.2920 - acc: 0.9057\n",
      "Epoch 118/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3156 - acc: 0.9057\n",
      "Epoch 119/200\n",
      "488/488 [==============================] - 0s 111us/step - loss: 0.3053 - acc: 0.8996\n",
      "Epoch 120/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2520 - acc: 0.9160\n",
      "Epoch 121/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2829 - acc: 0.9242\n",
      "Epoch 122/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2778 - acc: 0.9057\n",
      "Epoch 123/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3399 - acc: 0.8811\n",
      "Epoch 124/200\n",
      "488/488 [==============================] - 0s 131us/step - loss: 0.2647 - acc: 0.9078\n",
      "Epoch 125/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3011 - acc: 0.9037\n",
      "Epoch 126/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2855 - acc: 0.9016\n",
      "Epoch 127/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.3076 - acc: 0.8975\n",
      "Epoch 128/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3120 - acc: 0.8975\n",
      "Epoch 129/200\n",
      "488/488 [==============================] - 0s 124us/step - loss: 0.3184 - acc: 0.8873\n",
      "Epoch 130/200\n",
      "488/488 [==============================] - 0s 130us/step - loss: 0.3125 - acc: 0.8975\n",
      "Epoch 131/200\n",
      "488/488 [==============================] - 0s 123us/step - loss: 0.2898 - acc: 0.9037\n",
      "Epoch 132/200\n",
      "488/488 [==============================] - 0s 124us/step - loss: 0.2576 - acc: 0.9160\n",
      "Epoch 133/200\n",
      "488/488 [==============================] - 0s 121us/step - loss: 0.3015 - acc: 0.8975\n",
      "Epoch 134/200\n",
      "488/488 [==============================] - 0s 129us/step - loss: 0.3182 - acc: 0.8893\n",
      "Epoch 135/200\n",
      "488/488 [==============================] - 0s 130us/step - loss: 0.2613 - acc: 0.9201\n",
      "Epoch 136/200\n",
      "488/488 [==============================] - 0s 123us/step - loss: 0.3044 - acc: 0.8975\n",
      "Epoch 137/200\n",
      "488/488 [==============================] - 0s 125us/step - loss: 0.2993 - acc: 0.8996\n",
      "Epoch 138/200\n",
      "488/488 [==============================] - 0s 122us/step - loss: 0.2529 - acc: 0.9283\n",
      "Epoch 139/200\n",
      "488/488 [==============================] - 0s 123us/step - loss: 0.2742 - acc: 0.9098\n",
      "Epoch 140/200\n",
      "488/488 [==============================] - 0s 127us/step - loss: 0.2943 - acc: 0.9078\n",
      "Epoch 141/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2763 - acc: 0.9016\n",
      "Epoch 142/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2751 - acc: 0.9057\n",
      "Epoch 143/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2611 - acc: 0.9221\n",
      "Epoch 144/200\n",
      "488/488 [==============================] - 0s 112us/step - loss: 0.3244 - acc: 0.8893\n",
      "Epoch 145/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2776 - acc: 0.9078\n",
      "Epoch 146/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2608 - acc: 0.9160\n",
      "Epoch 147/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2575 - acc: 0.9201\n",
      "Epoch 148/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.3102 - acc: 0.8914\n",
      "Epoch 149/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.2960 - acc: 0.8893\n",
      "Epoch 150/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.3041 - acc: 0.8934\n",
      "Epoch 151/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2865 - acc: 0.9016\n",
      "Epoch 152/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2750 - acc: 0.9098\n",
      "Epoch 153/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2476 - acc: 0.9201\n",
      "Epoch 154/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2846 - acc: 0.9078\n",
      "Epoch 155/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2761 - acc: 0.9098\n",
      "Epoch 156/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2500 - acc: 0.9201\n",
      "Epoch 157/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2766 - acc: 0.9139\n",
      "Epoch 158/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2562 - acc: 0.9119\n",
      "Epoch 159/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.2536 - acc: 0.9242\n",
      "Epoch 160/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.2431 - acc: 0.9221\n",
      "Epoch 161/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.2541 - acc: 0.9180\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488/488 [==============================] - 0s 113us/step - loss: 0.2511 - acc: 0.9201\n",
      "Epoch 163/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2358 - acc: 0.9242\n",
      "Epoch 164/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.3098 - acc: 0.8852\n",
      "Epoch 165/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2811 - acc: 0.9098\n",
      "Epoch 166/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.2479 - acc: 0.9242\n",
      "Epoch 167/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.2906 - acc: 0.9078\n",
      "Epoch 168/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.2944 - acc: 0.8975\n",
      "Epoch 169/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.3043 - acc: 0.8934\n",
      "Epoch 170/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.2876 - acc: 0.9037\n",
      "Epoch 171/200\n",
      "488/488 [==============================] - 0s 124us/step - loss: 0.2314 - acc: 0.9283\n",
      "Epoch 172/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2177 - acc: 0.9324\n",
      "Epoch 173/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2525 - acc: 0.9221\n",
      "Epoch 174/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2558 - acc: 0.9180\n",
      "Epoch 175/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2331 - acc: 0.9324\n",
      "Epoch 176/200\n",
      "488/488 [==============================] - 0s 114us/step - loss: 0.2209 - acc: 0.9324\n",
      "Epoch 177/200\n",
      "488/488 [==============================] - 0s 113us/step - loss: 0.2268 - acc: 0.9344\n",
      "Epoch 178/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2617 - acc: 0.9201\n",
      "Epoch 179/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2635 - acc: 0.9180\n",
      "Epoch 180/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2649 - acc: 0.9119\n",
      "Epoch 181/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.3199 - acc: 0.8832\n",
      "Epoch 182/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2990 - acc: 0.8975\n",
      "Epoch 183/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2857 - acc: 0.9037\n",
      "Epoch 184/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2695 - acc: 0.9160\n",
      "Epoch 185/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2741 - acc: 0.9160\n",
      "Epoch 186/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2577 - acc: 0.9201\n",
      "Epoch 187/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2485 - acc: 0.9262\n",
      "Epoch 188/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2647 - acc: 0.9139\n",
      "Epoch 189/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2620 - acc: 0.9078\n",
      "Epoch 190/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2944 - acc: 0.8934\n",
      "Epoch 191/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2897 - acc: 0.8955\n",
      "Epoch 192/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2546 - acc: 0.9160\n",
      "Epoch 193/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2645 - acc: 0.9098\n",
      "Epoch 194/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2553 - acc: 0.9139\n",
      "Epoch 195/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2752 - acc: 0.9098\n",
      "Epoch 196/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2476 - acc: 0.9180\n",
      "Epoch 197/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2832 - acc: 0.9037\n",
      "Epoch 198/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.3111 - acc: 0.8893\n",
      "Epoch 199/200\n",
      "488/488 [==============================] - 0s 116us/step - loss: 0.2823 - acc: 0.9016\n",
      "Epoch 200/200\n",
      "488/488 [==============================] - 0s 115us/step - loss: 0.2839 - acc: 0.8955\n",
      "('outDir: ', '/home/kuan-hao/EPPC/output/TEST//TEST/all_pos_neg_training')\n",
      "     ** Plot confusion_matrix: \n",
      "     ** Plot normalized confusion_matrix: \n",
      "     ** Plot curve_roc: \n",
      "     ** Plot curve_pr: \n",
      "Final Model Evaluation!\n",
      "('outDir: ', '/home/kuan-hao/EPPC/output/TEST//TEST/all_pos_neg')\n",
      "     ** Plot confusion_matrix: \n",
      "     ** Plot normalized confusion_matrix: \n",
      "     ** Plot curve_roc: \n",
      "     ** Plot curve_pr: \n",
      "to predict: 81609\n",
      "('pred_class: ', 81609)\n",
      "('out: ', 81609)\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "## This part is \"MODEL TRAINING & PREDICTION\"\n",
    "##############################################\n",
    "network = utils.make_predictions(scoreCalc, mode, clf, gs, output_dir, fun_anno=functionalData, verbose = False, k_d_training = K_D_TRAIN, folds = FOLD_NUM, train_test_ratio = TRAIN_TEST_RATIO)\n",
    "\n",
    "# Predict protein interaction\n",
    "outFH = open(\"%s.pred.txt\" % (output_dir), \"w\")\n",
    "\n",
    "final_network = []\n",
    "for PPI in network:\n",
    "    items = PPI.split(\"\\t\")\n",
    "    if float(items[2]) >= classifier_cutoff:\n",
    "    # if float(items[2]) >= 0.0:\n",
    "        final_network.append(PPI)\n",
    "\n",
    "print >> outFH, \"\\n\".join(final_network)\n",
    "outFH.close()\n",
    "#############################################\n",
    "## End of \"MODEL TRAINING & PREDICTION\"\n",
    "#############################################\n",
    "\n",
    "# Predicting clusters\n",
    "utils.predict_clusters(\"%s.pred.txt\" % (output_dir), \"%s.clust.txt\" % (output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average size of predicted complexes is: 476.0\n",
      "# of complexes in reference dataset: 0\n",
      "mmr\t0.00945378151261\n",
      "overlapp\t0.000000\n",
      "simcoe\t1.000000\n",
      "mean_simcoe_overlap\t1.000000\n",
      "sensetivity\t1.0\n",
      "ppv\t0.0873015873015873\n",
      "accuracy\t0.295468420143\n",
      "sep\t0.188982236505\n"
     ]
    }
   ],
   "source": [
    "# Evaluating predicted clusters\n",
    "pred_clusters = GS.Clusters(False)\n",
    "pred_clusters.read_file(\"%s.clust.txt\" % (output_dir))\n",
    "overlapped_complexes_with_reference = gs.get_complexes().get_overlapped_complexes_set(pred_clusters)\n",
    "print \"# of complexes in reference dataset: \" + str(len(overlapped_complexes_with_reference))\n",
    "#clust_scores, header = utils.clustering_evaluation(gs.complexes, pred_clusters, \"\", False)\n",
    "clust_scores, header, composite_score = utils.clustering_evaluation(gs.complexes, pred_clusters, \"\", False)\n",
    "outFH = open(\"%s.eval.txt\" % (output_dir), \"w\")\n",
    "header = header.split(\"\\t\")\n",
    "clust_scores = clust_scores.split(\"\\t\")\n",
    "for i, head in enumerate(header):\n",
    "    print \"%s\\t%s\" % (head, clust_scores[i])\n",
    "    print >> outFH, \"%s\\t%s\" % (head, clust_scores[i])\n",
    "outFH.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
